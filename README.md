<table>
  <tr>
    <td align="center">
      <img src="detect_preference_gpt4_updated.png" alt="Preference Alignment" width="800"><br>
      <strong>Preference Alignment</strong>
    </td>
    <td align="center">
      <img src="detect_hateful_gpt4_updated.png" alt="Hatefulness" width="800"><br>
      <strong>Hatefulness</strong>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="detect_offensive_gpt4_updated.png" alt="Offensiveness" width="800"><br>
      <strong>Offensiveness</strong>
    </td>
    <td align="center">
      <img src="detect_toxic_gpt4_updated.png" alt="Toxicity" width="800"><br>
      <strong>Toxicity</strong>
    </td>
  </tr>
</table>

<p align="center"><em>Fig. R1: (Updated Figures 3 & 6) The AUC of different methods for four datasets with the total fraction of cheaters fixed at 0.25. FaitCrowd and AggNet are the two suggested baselines. AggNet only works for binary labels which does not directly apply to the preference alignment dataset.</em></p>


<table>
  <tr>
    <td align="center">
      <img src="detect_preference_gpt4_updated_02.png" alt="Preference Alignment" width="800"><br>
      <strong>Preference Alignment</strong>
    </td>
    <td align="center">
      <img src="detect_hateful_gpt4_updated_02.png" alt="Hatefulness" width="800"><br>
      <strong>Hatefulness</strong>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="detect_offensive_gpt4_updated_02.png" alt="Offensiveness" width="800"><br>
      <strong>Offensiveness</strong>
    </td>
    <td align="center">
      <img src="detect_toxic_gpt4_updated_02.png" alt="Toxicity" width="800"><br>
      <strong>Toxicity</strong>
    </td>
  </tr>
</table>

<p align="center"><em>Fig. R2: The same as Fig. R1 while fixing the total fraction of cheaters at 0.2</em></p>


<table>
  <tr>
    <td align="center">
      <img src="detect_preference_gpt4_updated_03.png" alt="Preference Alignment" width="800"><br>
      <strong>Preference Alignment</strong>
    </td>
    <td align="center">
      <img src="detect_hateful_gpt4_updated_03.png" alt="Hatefulness" width="800"><br>
      <strong>Hatefulness</strong>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="detect_offensive_gpt4_updated_03.png" alt="Offensiveness" width="800"><br>
      <strong>Offensiveness</strong>
    </td>
    <td align="center">
      <img src="detect_toxic_gpt4_updated_03.png" alt="Toxicity" width="800"><br>
      <strong>Toxicity</strong>
    </td>
  </tr>
</table>

<p align="center"><em>Fig. R3: The same as Fig. R1 while fixing the total fraction of cheaters at 0.3</em></p>


<table>
  <tr>
    <td align="center">
      <img src="detect_preference_gpt3_gpt4.png" alt="Preference Alignment" width="600"><br>
      <strong>GPT-3.5 & GPT-4</strong>
    </td>
    <td align="center">
      <img src="detect_preference_gemma_gpt4.png" alt="Hatefulness" width="600"><br>
      <strong>Gemma & GPT-4</strong>
    </td>
    <td align="center">
      <img src="detect_preference_gemma_gpt3.png" alt="Hatefulness" width="600"><br>
      <strong>Gemma & GPT-3.5</strong>
    </td>
  </tr>
</table>

<p align="center"><em>Fig. R4: The AUC of different methods for the preference alignment dataset with the total fraction of cheaters fixed at 0.25 under the mixed LLM cheating setting. Each LLM cheater applies one of the two LLMs with equal probability.</em></p>
